{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb3cfe3",
   "metadata": {},
   "source": [
    "# Example of using SpecialtyInsurance Simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5ae1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Set Simulation Parameters\n",
    "\n",
    "import os\n",
    "from logger.arguments import get_arguments\n",
    "\n",
    "# Read arguments from logger.arguments\n",
    "sim_args, manager_args, broker_args, syndicate_args, reinsurancefirm_args, shareholder_args, risk_args = get_arguments()\n",
    "\n",
    "# Reset arguments\n",
    "sim_args[\"max_time\"] = 30   # Simulation time span unit day\n",
    "manager_args[\"lead_top_k\"] = 3   # Number of syndicates competing for the lead quote\n",
    "manager_args[\"follow_top_k\"] = 2   # Number of syndicates following the lead strategy\n",
    "broker_args[\"num_brokers\"] = 1   # Number of brokers in the insurance market\n",
    "syndicate_args[\"num_syndicates\"] = 3   # Number of syndicates in the insurance market\n",
    "shareholder_args[\"num_shareholders\"] = 1   # Number of shareholders in the insurance market\n",
    "risk_args[\"num_risks\"] = 1  # Number of risks\n",
    "risk_args[\"num_categories\"] = 4  # Number of risk categories\n",
    "\n",
    "# No reinsurance mechanism included in this stage\n",
    "with_reinsurance = False   \n",
    "\n",
    "# Nomber of risk models loaded to all syndicates\n",
    "num_risk_models = 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b158de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catastrophe starts at time 10\n"
     ]
    }
   ],
   "source": [
    "### 2. Generate Catastrophes\n",
    "\n",
    "from environment.risk_generator import RiskGenerator\n",
    "\n",
    "# Create catastrophe list and catastrophe configurations\n",
    "catastrophes, risk_model_configs = RiskGenerator(num_risk_models, sim_args, risk_args).generate_risks()\n",
    "print(\"catastrophe starts at time\", catastrophes[0].get(\"risk_start_time\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f8ba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broker_id': '0', 'broker_risk': [], 'broker_quote': []}\n",
      "{'syndicate_id': '0', 'initial_capital': 10000000, 'current_capital': 10000000, 'premium_internal_weight': 0.5, 'interest_rate': 0.001, 'play_leader_in_contracts': [], 'play_follower_in_contracts': [], 'loss_experiency_weight': 0.2, 'volatility_weight': 0, 'underwriter_markup_recency_weight': 0.2, 'upper_premium_limit': 1.2, 'lower_premium_limit': 0.85, 'premium_reserve_ratio': 0.5, 'minimum_capital_reserve_ratio': 1, 'maximum_scaling_factor': 1, 'market_entry_probability': 0.3, 'exit_capital_threshold': 0.6, 'exit_time_limit': 24, 'premium_sensitivity': 5, 'acceptance_threshold_friction': 0.9}\n",
      "{'syndicate_id': '1', 'initial_capital': 10000000, 'current_capital': 10000000, 'premium_internal_weight': 0.5, 'interest_rate': 0.001, 'play_leader_in_contracts': [], 'play_follower_in_contracts': [], 'loss_experiency_weight': 0.2, 'volatility_weight': 0, 'underwriter_markup_recency_weight': 0.2, 'upper_premium_limit': 1.2, 'lower_premium_limit': 0.85, 'premium_reserve_ratio': 0.5, 'minimum_capital_reserve_ratio': 1, 'maximum_scaling_factor': 1, 'market_entry_probability': 0.3, 'exit_capital_threshold': 0.6, 'exit_time_limit': 24, 'premium_sensitivity': 5, 'acceptance_threshold_friction': 0.9}\n",
      "{'syndicate_id': '2', 'initial_capital': 10000000, 'current_capital': 10000000, 'premium_internal_weight': 0.5, 'interest_rate': 0.001, 'play_leader_in_contracts': [], 'play_follower_in_contracts': [], 'loss_experiency_weight': 0.2, 'volatility_weight': 0, 'underwriter_markup_recency_weight': 0.2, 'upper_premium_limit': 1.2, 'lower_premium_limit': 0.85, 'premium_reserve_ratio': 0.5, 'minimum_capital_reserve_ratio': 1, 'maximum_scaling_factor': 1, 'market_entry_probability': 0.3, 'exit_capital_threshold': 0.6, 'exit_time_limit': 24, 'premium_sensitivity': 5, 'acceptance_threshold_friction': 0.9}\n"
     ]
    }
   ],
   "source": [
    "### 3. Generate Insurance Market\n",
    "\n",
    "from environment.market_generator import MarketGenerator\n",
    "\n",
    "# Create lists of brokers, syndicates, reinsurancefirms, and shareholders\n",
    "brokers, syndicates, reinsurancefirms, shareholders = MarketGenerator(with_reinsurance, \n",
    "                                                                      num_risk_models, \n",
    "                                                                      sim_args, \n",
    "                                                                      broker_args, \n",
    "                                                                      syndicate_args, \n",
    "                                                                      reinsurancefirm_args, \n",
    "                                                                      shareholder_args, \n",
    "                                                                      risk_model_configs).generate_agents()\n",
    "for broker_id in range(len(brokers)):\n",
    "    print(brokers[broker_id].data())\n",
    "for syndicate_id in range(len(syndicates)):\n",
    "    print(syndicates[syndicate_id].data())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dfc0a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk_id: 0 broker_id: 0 risk_start_time: 0 risk_end_time: 360 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 1 broker_id: 0 risk_start_time: 1 risk_end_time: 361 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 2 broker_id: 0 risk_start_time: 2 risk_end_time: 362 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 3 broker_id: 0 risk_start_time: 3 risk_end_time: 363 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 4 broker_id: 0 risk_start_time: 4 risk_end_time: 364 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 5 broker_id: 0 risk_start_time: 5 risk_end_time: 365 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 6 broker_id: 0 risk_start_time: 6 risk_end_time: 366 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 7 broker_id: 0 risk_start_time: 7 risk_end_time: 367 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 8 broker_id: 0 risk_start_time: 8 risk_end_time: 368 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 9 broker_id: 0 risk_start_time: 9 risk_end_time: 369 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 10 broker_id: 0 risk_start_time: 10 risk_end_time: 370 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 11 broker_id: 0 risk_start_time: 11 risk_end_time: 371 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 12 broker_id: 0 risk_start_time: 12 risk_end_time: 372 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 13 broker_id: 0 risk_start_time: 13 risk_end_time: 373 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 14 broker_id: 0 risk_start_time: 14 risk_end_time: 374 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 15 broker_id: 0 risk_start_time: 15 risk_end_time: 375 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 16 broker_id: 0 risk_start_time: 16 risk_end_time: 376 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 17 broker_id: 0 risk_start_time: 17 risk_end_time: 377 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 18 broker_id: 0 risk_start_time: 18 risk_end_time: 378 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 19 broker_id: 0 risk_start_time: 19 risk_end_time: 379 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 20 broker_id: 0 risk_start_time: 20 risk_end_time: 380 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 21 broker_id: 0 risk_start_time: 21 risk_end_time: 381 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 22 broker_id: 0 risk_start_time: 22 risk_end_time: 382 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 23 broker_id: 0 risk_start_time: 23 risk_end_time: 383 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 24 broker_id: 0 risk_start_time: 24 risk_end_time: 384 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 25 broker_id: 0 risk_start_time: 25 risk_end_time: 385 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 26 broker_id: 0 risk_start_time: 26 risk_end_time: 386 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 27 broker_id: 0 risk_start_time: 27 risk_end_time: 387 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 28 broker_id: 0 risk_start_time: 28 risk_end_time: 388 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n",
      "risk_id: 29 broker_id: 0 risk_start_time: 29 risk_end_time: 389 risk_factor: 0.4481691027761267 risk_category: 2 risk_value: 1000.0\n"
     ]
    }
   ],
   "source": [
    "### 4. Input risk from broker\n",
    "\n",
    "from environment.event_generator import EventGenerator\n",
    "\n",
    "current_time = 0\n",
    "broker_risk_events = EventGenerator(risk_model_configs).generate_risk_events(sim_args, brokers, catastrophes)\n",
    "\n",
    "for i in range(len(broker_risk_events)):\n",
    "    print(\"risk_id:\", broker_risk_events[i].risk_id, \"broker_id:\", broker_risk_events[i].broker_id, \"risk_start_time:\", broker_risk_events[i].risk_start_time,\n",
    "         \"risk_end_time:\", broker_risk_events[i].risk_end_time, \"risk_factor:\", broker_risk_events[i].risk_factor,\n",
    "         \"risk_category:\", broker_risk_events[i].risk_category, \"risk_value:\", broker_risk_events[i].risk_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec86fe5-5568-43d0-bfe5-75632f83bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Create Multi-agent Environment to get access to the market performance\n",
    "\n",
    "import gym\n",
    "from environment.environment import SpecialtyInsuranceMarketEnv\n",
    "from environment.event_generator import EventGenerator\n",
    "from manager.ai_model.action import Action\n",
    "from manager import *\n",
    "\n",
    "class MultiAgentBasedModel(SpecialtyInsuranceMarketEnv):\n",
    "\n",
    "    def __init__(self, sim_args, manager_args, brokers, syndicates, reinsurancefirms, shareholders, catastrophes, risk_model_configs, with_reinsurance, num_risk_models, dt = 1):\n",
    "\n",
    "        self.sim_args = sim_args\n",
    "        self.maxstep = self.sim_args[\"max_time\"]\n",
    "        self.manager_args = manager_args\n",
    "        self.brokers = brokers\n",
    "        self.initial_brokers = brokers\n",
    "        self.syndicates = syndicates\n",
    "        self.initial_syndicates = syndicates\n",
    "        self.reinsurancefirms = reinsurancefirms\n",
    "        self.initial_reinsurancefirms = reinsurancefirms\n",
    "        self.shareholders = shareholders\n",
    "        self.initial_shareholders = shareholders\n",
    "        self.risks = catastrophes\n",
    "        self.initial_risks = catastrophes\n",
    "        self.risk_model_configs = risk_model_configs\n",
    "        self.with_reinsurance = with_reinsurance\n",
    "        self.num_risk_models = num_risk_models\n",
    "        self.dt = dt\n",
    "        self.mm = None\n",
    "        self.event_handler = None\n",
    "\n",
    "        # Active syndicate list\n",
    "        self.syndicate_active_list = []\n",
    "        # Initialise events, actions, and states \n",
    "        self.broker_risk_events = []\n",
    "        self.action_map_dict = {}\n",
    "        self.state_encoder_dict = {}\n",
    "\n",
    "        # Define Action Space, Define Observation Space\n",
    "        self.n = len(self.syndicates)\n",
    "        self.agents = {self.syndicates[i].syndicate_id for i in range(self.n)}\n",
    "        self._agent_ids = set(self.agents)\n",
    "        self.dones = set()\n",
    "        self._spaces_in_preferred_format = True\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            \"0\": gym.spaces.Box(low=-1000000,high=1000000,shape=(6,), dtype = np.float32),\n",
    "            \"1\": gym.spaces.Box(low=-1000000,high=1000000,shape=(6,), dtype = np.float32),\n",
    "            \"2\": gym.spaces.Box(low=-1000000,high=1000000,shape=(6,), dtype = np.float32)\n",
    "        })\n",
    "        self.action_space = gym.spaces.Dict({\n",
    "            \"0\": gym.spaces.Box(0.0, 0.9, dtype = np.float32),\n",
    "            \"1\": gym.spaces.Box(0.0, 0.9, dtype = np.float32),\n",
    "            \"2\": gym.spaces.Box(0.0, 0.9, dtype = np.float32)})\n",
    "\n",
    "        super(MultiAgentBasedModel, self).__init__(sim_args = self.sim_args, \n",
    "                                                   manager_args = self.manager_args , \n",
    "                                                   brokers = self.brokers, \n",
    "                                                   syndicates = self.syndicates, \n",
    "                                                   reinsurancefirms = self.reinsurancefirms, \n",
    "                                                   shareholders = self.shareholders, \n",
    "                                                   risks = self.risks, \n",
    "                                                   risk_model_configs = self.risk_model_configs, \n",
    "                                                   with_reinsurance = self.with_reinsurance, \n",
    "                                                   num_risk_models = self.num_risk_models,\n",
    "                                                   dt = 1)\n",
    "        # Reset the environmnet\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        super().reset(seed = seed)\n",
    "        \n",
    "        # Reset the environment to an initial state\n",
    "        self.brokers = self.initial_brokers\n",
    "        self.syndicates = self.initial_syndicates\n",
    "        self.reinsurancefirms = self.initial_reinsurancefirms\n",
    "        self.shareholders = self.initial_shareholders\n",
    "        self.risks = self.initial_risks\n",
    "        # Broker risk event daily: broker generate risk according to poisson distribution\n",
    "        self.broker_risk_events = broker_risk_events\n",
    "        self.event_handler = EventHandler(self.maxstep, self.broker_risk_events)\n",
    "        # Initiate market manager\n",
    "        self.mm = MarketManager(self.maxstep, self.manager_args, self.brokers, self.syndicates, self.reinsurancefirms, self.shareholders, self.risks, self.risk_model_configs, self.with_reinsurance, self.num_risk_models, self.broker_risk_events, self.event_handler)\n",
    "        self.mm.evolve(self.dt)\n",
    "        \n",
    "        # Set per syndicate active status and build status list\n",
    "        self.syndicate_active_list = []   # Store syndicates currently in the market\n",
    "        for syndicate_id in self.mm.market.syndicates:\n",
    "            if self.mm.market.syndicates[syndicate_id].status == True:\n",
    "                self.syndicate_active_list.append(syndicate_id)\n",
    "\n",
    "        # Create action map and state list\n",
    "        info_dict = {}\n",
    "        for syndicate_id in self.syndicate_active_list:\n",
    "            self.action_map_dict[syndicate_id] = self.action_map_creator(self.mm.market.syndicates[syndicate_id], 0)\n",
    "            #self.state_encoder_dict[syndicate_id] = self.state_encoder(syndicate_id)\n",
    "            self.state_encoder_dict = {i: self.observation_space[i].sample() for i in self.agents}\n",
    "            info_dict[syndicate_id] = self._get_info()\n",
    "\n",
    "        # Initiate time step\n",
    "        self.timestep = -1\n",
    "        self.step_track = 0\n",
    "        self.log = []\n",
    "\n",
    "        return self.state_encoder_dict, info_dict\n",
    "\n",
    "    def step(self, action_dict):\n",
    "\n",
    "        obs_dict, reward_dict, terminated_dict, info_dict = {}, {}, {}, {}\n",
    "        flag_dict = {}\n",
    "\n",
    "        # Update environemnt after actions\n",
    "        parsed_actions = []        \n",
    "        for syndicate_id, action in action_dict.items():\n",
    "            # update action map\n",
    "            self.action_map = self.action_map_creator(self.mm.market.syndicates[syndicate_id],action)\n",
    "            parsed_ac2add = self.action_map\n",
    "            parsed_actions.append(parsed_ac2add)\n",
    "        \n",
    "        self.send_action2env(parsed_actions)\n",
    "\n",
    "        # Update broker_risk_events, broker_premium_events, broker_claim_events, event_handler, market manager\n",
    "        \"\"\"self.broker_premium_events = EventGenerator(self.risk_model_configs).generate_premium_events(self.brokers, self.timestep)\n",
    "        self.event_handler.add_premium_events(self.broker_premium_events)\n",
    "        for i in range(len(self.catastrophe_events)):\n",
    "            if self.catastrophe_events[i].risk_start_time == self.timestep:\n",
    "                self.broker_claim_events = EventGenerator(self.risk_model_configs).generate_claim_events(self.brokers, self.timestep)\n",
    "                self.event_handler.add_claim_events(self.broker_claim_events)\n",
    "        self.mm.update_premium_events(self.broker_premium_events, self.event_handler)\n",
    "        self.mm.update_claim_events(self.broker_claim_events, self.event_handler)\"\"\"\n",
    "\n",
    "        \n",
    "        market = self.mm.evolve(self.dt)\n",
    "        self.timestep += 1\n",
    "        print(self.timestep)\n",
    "\n",
    "        # Compute rewards and get next observation\n",
    "        for syndicate_id, action in action_dict.items():\n",
    "            reward_dict[syndicate_id] = self.compute_reward(action, syndicate_id)\n",
    "            obs_dict[syndicate_id]= self.state_encoder(syndicate_id)\n",
    "            info_dict[syndicate_id] = self._get_info()\n",
    "            flag_dict[syndicate_id] = False\n",
    "        \n",
    "        # Check termination\n",
    "        for syndicate_id in self.syndicate_active_list:\n",
    "            terminated_dict[syndicate_id] = self.check_termination(syndicate_id)\n",
    "\n",
    "        # Update plot \n",
    "        self.draw2file(market)\n",
    "\n",
    "        # All done termination check\n",
    "        all_terminated = True\n",
    "        for _, syndicate_terminated in terminated_dict.items():\n",
    "            if syndicate_terminated is False:\n",
    "                all_terminated = False\n",
    "                break\n",
    "        \n",
    "        terminated_dict[\"__all__\"] = all_terminated\n",
    "        flag_dict[\"__all__\"] = all_terminated\n",
    "        print(obs_dict, reward_dict, terminated_dict)\n",
    "\n",
    "        return obs_dict, reward_dict, terminated_dict, flag_dict, info_dict\n",
    "\n",
    "    def compute_reward(self, action, syndicate_id):\n",
    "\n",
    "        market = self.mm.market\n",
    "        # calculate reward function\n",
    "        r = [0.0] * 4\n",
    "\n",
    "        # For each insurable risk being accepted +1 or refused -1\n",
    "        if(self.timestep <= self.maxstep):\n",
    "            for broker_id in range(len(market.brokers)):\n",
    "                for risk in range(len(market.brokers[broker_id].risks)):\n",
    "                    for contract in range(len(market.brokers[broker_id].underwritten_contracts)):\n",
    "                        if market.brokers[broker_id].risks[risk][\"risk_id\"] == market.brokers[broker_id].underwritten_contracts[contract][\"risk_id\"]:\n",
    "                            r[0] += 1\n",
    "                        else:\n",
    "                            r[0] -= 1\n",
    "\n",
    "        # For each claim being paied +1 or refused -1\n",
    "        if(self.timestep <= self.maxstep):\n",
    "            for claim in range(len(market.syndicate[syndicate_id].paid_claim)):\n",
    "                if market.syndicate[syndicate_id].paid_claim[claim][\"status\"] == True:\n",
    "                    r[1] += 1\n",
    "                else:\n",
    "                    r[1] -= 1\n",
    "\n",
    "        # Profit and Bankruptcy       \n",
    "        if(self.timestep <= self.maxstep):\n",
    "            initial_capital = self.syndicate_status[syndicate_id].initial_capital\n",
    "            current_capital = self.syndicate_status[syndicate_id].update_capital()\n",
    "            r[2] += current_capital - initial_capital\n",
    "            if (current_capital - initial_capital) < 0:\n",
    "                r[3] -= 10000\n",
    "\n",
    "        # Sum reward\n",
    "        reward = 0.0\n",
    "        reward += np.sum(r)\n",
    "\n",
    "        return reward     \n",
    "    \n",
    "    def state_encoder(self, syndicate_id):\n",
    "\n",
    "        ### Observation Space:             \n",
    "        obs = []\n",
    "        market = self.mm.market\n",
    "        for risk in range(len(market.broker_bring_risk)):\n",
    "            if market.broker_bring_risk[risk][\"risk_start_time\"] == self.timestep:\n",
    "                # Catastrophe risk category and risk value\n",
    "                obs.append(market.broker_bring_risk[risk][\"risk_category\"])\n",
    "                obs.append(market.broker_bring_risk[risk][\"risk_value\"])\n",
    "        \n",
    "        # Syndicates status current capital in \n",
    "        for num in range(len(self.syndicates[syndicate_id].current_capital_category)):\n",
    "            obs.append(self.syndicates[syndicate_id].current_capital_category[num])\n",
    "            \n",
    "        return obs\n",
    "\n",
    "    def obs_space_creator(self):\n",
    "        # risk_category, risk_value, current capital for each category for each syndicate\n",
    "        low, high = [], []\n",
    "        low.extend([0.0, 0.0, -10000000, -10000000, -10000000, -10000000])\n",
    "        high.extend([10.0, 10000000, 30000000, 30000000, 30000000, 30000000])\n",
    "        observation_space = gym.spaces.Box(np.array(low, dtype=np.float32), \n",
    "                                       np.array(high, dtype=np.float32)) \n",
    "        return observation_space\n",
    "\n",
    "    def action_map_creator(self, syndicate, line_size):\n",
    "\n",
    "        for r in range(len(syndicate.received_risk_list)):\n",
    "            if syndicate.received_risk_list[\"start_time\"] == self.timestep:\n",
    "                action_map = Action(syndicate.syndicate_id, line_size, syndicate.received_risk[r][\"risk_id\"], syndicate.received_risk[r][\"broker_id\"])\n",
    "       \n",
    "        return action_map\n",
    "\n",
    "    def set_action_space(self):\n",
    "        return gym.spaces.Box(0.0, 0.9, dtype = np.float32)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a013d49-4182-43a5-8317-3e44e174373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k2472543/Library/Python/3.8/lib/python/site-packages/ray/rllib/algorithms/algorithm.py:483: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/k2472543/Library/Python/3.8/lib/python/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/k2472543/Library/Python/3.8/lib/python/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/k2472543/Library/Python/3.8/lib/python/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 GPUs on your machine (GPU devices found: [])! If your\n    machine does not have any GPUs, you should set the config keys `num_gpus` and\n    `num_gpus_per_worker` to 0 (they may be set to 1 by default for your\n    particular RL algorithm).\nTo change the config for the `rllib train|rollout` command, use\n  `--config={'[key]': '[value]'}` on the command line.\nTo change the config for `tune.Tuner().fit()` in a script: Modify the python dict\n  passed to `tune.Tuner(param_space=[...]).fit()`.\nTo change the config for an RLlib Algorithm instance: Modify the python dict\n  passed to the Algorithm's constructor, e.g. `PPO(config=[...])`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# The number of training iteration for the RL agent\u001b[39;00m\n\u001b[1;32m     66\u001b[0m num_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 68\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsurance_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Number of training iterations\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_training):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Create a path to store the trained agent for each iteration\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 56\u001b[0m, in \u001b[0;36mppo_trainer_creator\u001b[0;34m(insurance_args)\u001b[0m\n\u001b[1;32m     34\u001b[0m     high\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m30000000.0\u001b[39m)\n\u001b[1;32m     36\u001b[0m config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     37\u001b[0m     PPOConfig()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;241m.\u001b[39menvironment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecialtyInsuranceMarket-validation\u001b[39m\u001b[38;5;124m\"\u001b[39m, env_config\u001b[38;5;241m=\u001b[39minsurance_args)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m.\u001b[39mresources(num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/ray/rllib/algorithms/algorithm.py:516\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     },\n\u001b[1;32m    514\u001b[0m }\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/ray/tune/trainable/trainable.py:161\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    157\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/ray/rllib/algorithms/algorithm.py:638\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Create a set of env runner actors via a WorkerSet.\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rollout_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/ray/rllib/evaluation/worker_set.py:159\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/ray/rllib/evaluation/worker_set.py:249\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runner_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/ray/rllib/evaluation/worker_set.py:950\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_worker\u001b[39m(\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    949\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[EnvRunner, ActorHandle]:\n\u001b[0;32m--> 950\u001b[0m     worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecreated_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecreated_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ds_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/ray/rllib/evaluation/rollout_worker.py:512\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, config, worker_index, num_workers, recreated_worker, log_dir, spaces, default_policy_class, dataset_shards, tf_session_creator)\u001b[0m\n\u001b[1;32m    509\u001b[0m         devices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()))\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devices) \u001b[38;5;241m<\u001b[39m num_gpus:\n\u001b[0;32m--> 512\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m             ERR_MSG_NO_GPUS\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(devices), devices)\n\u001b[1;32m    514\u001b[0m             \u001b[38;5;241m+\u001b[39m HOWTO_CHANGE_CONFIG\n\u001b[1;32m    515\u001b[0m         )\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Warn, if running in local-mode and actual GPUs (not faked) are\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# requested.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    519\u001b[0m     ray\u001b[38;5;241m.\u001b[39mis_initialized()\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_mode() \u001b[38;5;241m==\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mLOCAL_MODE\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m num_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_fake_gpus\n\u001b[1;32m    523\u001b[0m ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 GPUs on your machine (GPU devices found: [])! If your\n    machine does not have any GPUs, you should set the config keys `num_gpus` and\n    `num_gpus_per_worker` to 0 (they may be set to 1 by default for your\n    particular RL algorithm).\nTo change the config for the `rllib train|rollout` command, use\n  `--config={'[key]': '[value]'}` on the command line.\nTo change the config for `tune.Tuner().fit()` in a script: Modify the python dict\n  passed to `tune.Tuner(param_space=[...]).fit()`.\nTo change the config for an RLlib Algorithm instance: Modify the python dict\n  passed to the Algorithm's constructor, e.g. `PPO(config=[...])`.\n"
     ]
    }
   ],
   "source": [
    "### 6. Register environment and train the model\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig, PPOTF1Policy, PPOTF2Policy, PPOTorchPolicy\n",
    "from ipywidgets import IntProgress\n",
    "from gym.spaces import Box\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "insurance_args = {\"sim_args\": sim_args,\n",
    "    \"manager_args\": manager_args,\n",
    "    \"brokers\": brokers,\n",
    "    \"syndicates\": syndicates,\n",
    "    \"reinsurancefirms\": reinsurancefirms,\n",
    "    \"shareholders\": shareholders,\n",
    "    \"catastrophes\": catastrophes,\n",
    "    \"risk_model_configs\": risk_model_configs,\n",
    "    \"with_reinsurance\": with_reinsurance,\n",
    "    \"num_risk_models\": num_risk_models}\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return MultiAgentBasedModel(**env_config)\n",
    "\n",
    "def ppo_trainer_creator(insurance_args):\n",
    "    low, high = [], []\n",
    "    n = len(syndicates)\n",
    "    low.extend([0.0, 0.0])\n",
    "    high.extend([10.0, 10000000.0]) # Number of risk category, risk limit, current capital\n",
    "    for num in range(risk_model_configs[0][\"num_categories\"]):\n",
    "        low.append(-10000000.0)\n",
    "        high.append(30000000.0)\n",
    "\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(\"SpecialtyInsuranceMarket-validation\", env_config=insurance_args)\n",
    "        .framework(\"tf\")\n",
    "        .multi_agent(policies={\n",
    "                # The Policy we are actually learning.\n",
    "                \"learnable_policy\": PolicySpec(\n",
    "                    config=PPOConfig.overrides(framework_str=\"tf\"),\n",
    "                    observation_space=gym.spaces.Box(low=-1000000,high=1000000,shape=(6,), dtype = np.float32),\n",
    "                    action_space=gym.spaces.Box(0.0, 0.9, dtype = np.float32)\n",
    "                ),\n",
    "            }, \n",
    "                     policy_mapping_fn=lambda agent_id, *args, **kwargs: [\n",
    "                \"learnable_policy\",\n",
    "            ][agent_id % 1],\n",
    "                     policies_to_train=[\"learnable_policy\"],\n",
    "                    )\n",
    "        .resources(num_gpus=0)\n",
    "    )\n",
    "    \n",
    "    trainer = PPO(config=config)\n",
    "    return trainer\n",
    "\n",
    "# Folder for recording\n",
    "top_dir = \"noreinsurance_\" + \"_model_\" + str(num_risk_models)\n",
    "\n",
    "# Register environment\n",
    "register_env(\"SpecialtyInsuranceMarket-validation\", env_creator)\n",
    "\n",
    "# The number of training iteration for the RL agent\n",
    "num_training = 10\n",
    "\n",
    "trainer = ppo_trainer_creator(insurance_args)\n",
    "# Number of training iterations\n",
    "\n",
    "for n in range(num_training):\n",
    "    # Create a path to store the trained agent for each iteration\n",
    "    model_filepath = f\"{top_dir}/{str(n)}/saved_models\"\n",
    "        \n",
    "    num_episode = 10\n",
    "\n",
    "    # A training iteration includes parallel sample collection by the environment workers \n",
    "    # as well as loss calculation on the collected batch and a model update.\n",
    "\n",
    "    bar = IntProgress(min=0, max=num_episode)\n",
    "    display(bar)\n",
    "    list_mean_rewards = []\n",
    "    list_min_rewards = []\n",
    "    list_max_rewards = []\n",
    "    list_train_step = []\n",
    "\n",
    "    for i in range(num_episode):\n",
    "        trainer.train()        \n",
    "        print(\"Progress:\", i+1, \"/\", num_episode, end=\"\\r\")\n",
    "        bar.value += 1\n",
    "        if (i+1) % config[\"evaluation_interval\"] == 0:\n",
    "            list_mean_rewards.append(trainer.evaluation_metrics[\"evaluation\"][\"episode_reward_mean\"])\n",
    "            list_min_rewards.append(trainer.evaluation_metrics[\"evaluation\"][\"episode_reward_min\"])\n",
    "            list_max_rewards.append(trainer.evaluation_metrics[\"evaluation\"][\"episode_reward_max\"])\n",
    "            list_train_step.append(i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33b840-6a04-40b9-ac1c-66f915f14bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Test the trained model performance\n",
    "\n",
    "def trainer_restore(self, top_dir, n):\n",
    "    if n <= 9:\n",
    "        path0 = top_dir\n",
    "        path1 = str(n-1)\n",
    "        path2 = \"saved_models\"\n",
    "        path3 = \"checkpoint_\"+str(0)+str(0)+str(0)+str(0)+str(0)+str(n)\n",
    "        path4 = \"rllib_checkpoint.json\"\n",
    "    elif 9 < n <= 99:\n",
    "        path0 = top_dir\n",
    "        path1 = str(n-1)\n",
    "        path2 = \"saved_models\"\n",
    "        path3 = \"checkpoint_\"+str(0)+str(0)+str(0)+str(0)+str(n)\n",
    "        path4 = \"rllib_checkpoint.json\"\n",
    "    elif 99 < n <= 999:\n",
    "        path0 = top_dir\n",
    "        path1 = str(n-1)\n",
    "        path2 = \"saved_models\"\n",
    "        path3 = \"checkpoint_\"+str(0)+str(0)+str(0)+str(n)\n",
    "        path4 = \"rllib_checkpoint.json\"\n",
    "\n",
    "    # Join various path components\n",
    "    self.trainer.restore(os.path.join(path0, path1, path2, path3, path4))\n",
    "\n",
    "insurance_args = {\"sim_args\": self.sim_args,\n",
    "        \"manager_args\": self.manager_args,\n",
    "        \"brokers\": self.brokers,\n",
    "        \"syndicates\": self.syndicates,\n",
    "        \"reinsurancefirms\": self.reinsurancefirms,\n",
    "        \"shareholders\": self.shareholders,\n",
    "        \"risks\": self.risks,\n",
    "        \"risk_model_configs\": self.risk_model_configs,\n",
    "         \"with_reinsurance\": self.with_reinsurance,\n",
    "        \"num_risk_models\": self.num_risk_models}\n",
    "\n",
    "validation_episodes = 1\n",
    "        \n",
    "for epi in range(validation_episodes):\n",
    "    env = MultiAgentBasedModel(**insurance_args)\n",
    "    \n",
    "    print(f\"\\nepisode: {epi} | \")\n",
    "    total_steps = 0\n",
    "    done = {\"__all__\": False}\n",
    "    all_rewards[epi] = {}\n",
    "    \n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done[\"__all__\"]:\n",
    "        if total_steps % 20 == 0: print(\".\", end=\"\")\n",
    "        \n",
    "        action_dict = trainer.compute_actions(obs)  \n",
    "        total_steps += 1\n",
    "        \n",
    "        obs, reward, done, info = env.step(action_dict, \n",
    "                                          draw_to_file=True)\n",
    "        for k, v in reward.items():\n",
    "            if k not in all_rewards[epi]:\n",
    "                all_rewards[epi][k] = [v]\n",
    "            else:\n",
    "                all_rewards[epi][k].append(v)\n",
    "\n",
    "        return all_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main function run the simulation, two syndicates will be chosen to compete for the leader position\n",
    "from manager.ai_model.runner import AIRunner\n",
    "from manager.game_model.runner import GameRunner\n",
    "\n",
    "model = 0\n",
    "if model == 0: \n",
    "    runner = AIRunner(sim_args, manager_args, brokers, syndicates, reinsurancefirms, shareholders, catastrophes, risk_model_configs, with_reinsurance, num_risk_models)\n",
    "elif model == 1:\n",
    "    runner = GameRunner(sim_args, manager_args, brokers, syndicates, reinsurancefirms, shareholders, catastrophes, risk_model_configs, with_reinsurance, num_risk_models)\n",
    "runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.event.add_catastrophe import AddCatastropheEvent\n",
    "from environment.event.add_attritionalloss import AddAttritionalLossEvent\n",
    "from environment.event.add_risk import AddRiskEvent\n",
    "from environment.event.add_premium import AddPremiumEvent\n",
    "from environment.event.add_claim import AddClaimEvent\n",
    "from environment.event_generator import EventGenerator\n",
    "from manager.event_handler import EventHandler\n",
    "from environment.market import NoReinsurance_RiskOne\n",
    "import numpy as np\n",
    "\n",
    "catastrophe_events = EventGenerator(risk_model_configs).generate_catastrophe_events(catastrophes)\n",
    "attritional_loss_events = EventGenerator(risk_model_configs).generate_attritional_loss_events(sim_args, catastrophes)\n",
    "broker_risk_events = broker_risk_events\n",
    "broker_premium_events = []\n",
    "broker_claim_events = []\n",
    "time = 0\n",
    "market = NoReinsurance_RiskOne(time, sim_args[\"max_time\"], manager_args, brokers, syndicates, \n",
    "                               shareholders, catastrophes, risk_model_configs, broker_risk_events)\n",
    "event_handler = EventHandler(sim_args[\"max_time\"], broker_risk_events)\n",
    "\n",
    "step_time = 1\n",
    "market_start_time = market.time\n",
    "market_end_time = market.time + step_time\n",
    "\n",
    "#upcoming_catastrophe = [\n",
    "            #e.risk_id for e in event_handler.upcoming_catastrophe.values() if isinstance(e, AddCatastropheEvent)\n",
    "        #]\n",
    "#upcoming_attritional_loss = [\n",
    "            #e.risk_id for e in event_handler.upcoming_attritional_loss.values() if isinstance(e, AddAttritionalLossEvent)\n",
    "        #]\n",
    "upcoming_broker_risk = [\n",
    "            e.risk_id for e in event_handler.upcoming_broker_risk.values() if isinstance(e, AddRiskEvent)\n",
    "        ]\n",
    "#upcoming_broker_premium = [\n",
    "            #e.risk_id for e in event_handler.upcoming_broker_premium.values() if isinstance(e, AddPremiumEvent)\n",
    "        #]\n",
    "#upcoming_broker_claim = [\n",
    "            #e.risk_id for e in event_handler.upcoming_broker_claim.values() if isinstance(e, AddClaimEvent)\n",
    "        #]\n",
    "step_time = 1\n",
    "market = event_handler.forward(market, step_time)\n",
    "\n",
    "#newly_added_catastrophe_events = {\n",
    "#            e.risk_id: e.risk_start_time\n",
    "#            for e in event_handler.completed_catastrophe.values()\n",
    "#            if isinstance(e, AddCatastropheEvent) and (e.risk_id in upcoming_catastrophe)\n",
    "#        }\n",
    "\n",
    "#newly_added_attritional_loss_events = {\n",
    "#            e.risk_id: e.risk_start_time\n",
    "#            for e in event_handler.completed_attritional_loss.values()\n",
    "#            if isinstance(e, AddAttritionalLossEvent) and (e.risk_id in upcoming_attritional_loss)\n",
    "#        }\n",
    "\n",
    "newly_added_broker_risk_events = {\n",
    "            e.risk_id: e.risk_start_time\n",
    "            for e in event_handler.completed_broker_risk.values()\n",
    "            if isinstance(e, AddRiskEvent) and (e.risk_id in upcoming_broker_risk)\n",
    "        }\n",
    "\n",
    "#newly_added_broker_premium_events = {\n",
    "#            e.risk_id: e.risk_start_time\n",
    "#            for e in event_handler.completed_broker_premium.values()\n",
    "#            if isinstance(e, AddPremiumEvent) and (e.risk_id in upcoming_broker_premium)\n",
    "#        }\n",
    "\n",
    "#newly_added_broker_claim_events = {\n",
    "#            e.risk_id: e.risk_start_time\n",
    "#            for e in event_handler.completed_broker_claim.values()\n",
    "#            if isinstance(e, AddClaimEvent) and (e.risk_id in upcoming_broker_claim)\n",
    "#        }\n",
    "\n",
    "#catastrophe_event_start_times = np.array(\n",
    "#            [\n",
    "#                newly_added_catastrophe_events.get(risk_id)\n",
    "#                for risk_id in upcoming_catastrophe\n",
    "#                if newly_added_catastrophe_events.get(risk_id) != None\n",
    "#            ]\n",
    "#        )\n",
    "\n",
    "#attritional_loss_event_start_times = np.array(\n",
    "#            [\n",
    "#                newly_added_attritional_loss_events.get(risk_id)\n",
    "#                for risk_id in upcoming_attritional_loss\n",
    "#                if newly_added_attritional_loss_events.get(risk_id) != None\n",
    "#            ]\n",
    "#        )\n",
    "\n",
    "broker_risk_event_start_times = np.array(\n",
    "            [\n",
    "                newly_added_broker_risk_events.get(risk_id)\n",
    "                for risk_id in upcoming_broker_risk\n",
    "                if newly_added_broker_risk_events.get(risk_id) != None\n",
    "            ]\n",
    "        )\n",
    "\n",
    "#broker_premium_event_start_times = np.array(\n",
    "#            [\n",
    "#                newly_added_broker_premium_events.get(risk_id)\n",
    "#                for risk_id in upcoming_broker_premium\n",
    "#                if newly_added_broker_premium_events.get(risk_id) != None\n",
    "#            ]\n",
    "#        )\n",
    "\n",
    "#broker_claim_event_start_times = np.array(\n",
    "#            [\n",
    "#                newly_added_broker_claim_events.get(risk_id)\n",
    "#                for risk_id in upcoming_broker_claim\n",
    "#                if newly_added_broker_claim_events.get(risk_id) != None\n",
    "#            ]\n",
    "#        )\n",
    "\n",
    "#event_start_times = np.concatenate((catastrophe_event_start_times,\n",
    "#                                    attritional_loss_event_start_times,\n",
    "#                                    broker_risk_event_start_times,\n",
    "#                                    broker_premium_event_start_times,\n",
    "#                                    broker_claim_event_start_times))\n",
    "\n",
    "sorted_unique_start_times = np.sort(np.unique(broker_risk_event_start_times))\n",
    "\n",
    "print(market.broker_bring_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ddd61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
