{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de846577",
   "metadata": {},
   "source": [
    "# Example of using SpecialtyInsurance Simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8474fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Simulation Parameters\n",
    "import os\n",
    "from logger.arguments import get_arguments\n",
    "\n",
    "# Read arguments from logger.arguments\n",
    "sim_args, manager_args, broker_args, syndicate_args, reinsurancefirm_args, shareholder_args, risk_args = get_arguments()\n",
    "\n",
    "# Reset arguments\n",
    "sim_args[\"max_time\"] = 300   # Simulation time span unit day\n",
    "manager_args[\"lead_top_k\"] = 2   # Number of syndicates competing for the lead quote\n",
    "manager_args[\"follow_top_k\"] = 1   # Number of syndicates following the lead strategy\n",
    "broker_args[\"num_brokers\"] = 1   # Number of brokers in the insurance market\n",
    "syndicate_args[\"num_syndicates\"] = 3   # Number of syndicates in the insurance market\n",
    "syndicate_args[\"lead_line_size\"] = 0.90  # The percentage of risk covered by lead syndicate\n",
    "syndicate_args[\"follow_line_size\"] = 0.10  # The percentage of risk covered by follow syndicate\n",
    "shareholder_args[\"num_shareholders\"] = 1   # Number of shareholders in the insurance market\n",
    "risk_args[\"num_risks\"] = 1  # Number of risk categories\n",
    "risk_args[\"num_categories\"] = 4  # Number of risk categories\n",
    "\n",
    "# No reinsurance mechanism included in this stage\n",
    "with_reinsurance = False   \n",
    "\n",
    "# Load one risk model to all syndicates\n",
    "num_risk_models = 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d683aac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Generate Catastrophes\n",
    "from environment.risk_generator import RiskGenerator\n",
    "\n",
    "# Create catastrophe list and catastrophe configurations\n",
    "catastrophes, risk_model_configs = RiskGenerator(num_risk_models, sim_args, risk_args).generate_risks()\n",
    "print(catastrophes[0].get(\"risk_start_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6e37e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Insurance Market\n",
    "from environment.market_generator import MarketGenerator\n",
    "\n",
    "# Create lists of brokers, syndicates, reinsurancefirms, and shareholders\n",
    "brokers, syndicates, reinsurancefirms, shareholders = MarketGenerator(with_reinsurance, \n",
    "                                                                      num_risk_models, \n",
    "                                                                      sim_args, \n",
    "                                                                      broker_args, \n",
    "                                                                      syndicate_args, \n",
    "                                                                      reinsurancefirm_args, \n",
    "                                                                      shareholder_args, \n",
    "                                                                      risk_model_configs).generate_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d0ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'risk_id': 0, 'broker_id': '0', 'risk_start_time': 11, 'risk_end_time': 371, 'risk_factor': 0.5, 'risk_category': 4, 'risk_value': 1000.0}]\n"
     ]
    }
   ],
   "source": [
    "# Input risk from broker\n",
    "\n",
    "# Risk brought by broker: time to the market and number of risks\n",
    "\n",
    "for i in range(len(brokers)):\n",
    "    risks = brokers[i].generate_risks(catastrophes)\n",
    "    print(risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef409dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:483: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/homebrew/lib/python3.8/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/homebrew/lib/python3.8/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/homebrew/lib/python3.8/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-03-04 14:48:18,253\tINFO worker.py:1724 -- Started a local Ray instance.\n",
      "2024-03-04 14:48:22,617\tERROR actor_manager.py:506 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n",
      "  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/environment/environment.py\", line 160, in step\n",
      "    market = self.mm.evolve(self.dt)\n",
      "  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/market_manager.py\", line 323, in evolve\n",
      "    self.market.time = start_time\n",
      "AttributeError: 'NoneType' object has no attribute 'time'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n",
      "    check_multiagent_environments(env)\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 368, in check_multiagent_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your environment (<SpecialtyInsuranceMarketEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "In particular, the `step()` method seems to be faulty.\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 414, in __init__\n",
      "    check_env(self.env, self.config)\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 96, in check_env\n",
      "    raise ValueError(\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 363, in check_multiagent_environments\n",
      "    results = env.step(sampled_action)\n",
      "  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/environment/environment.py\", line 160, in step\n",
      "    market = self.mm.evolve(self.dt)\n",
      "  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/market_manager.py\", line 323, in evolve\n",
      "    self.market.time = start_time\n",
      "AttributeError: 'NoneType' object has no attribute 'time'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n",
      "    check_multiagent_environments(env)\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 368, in check_multiagent_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your environment (<SpecialtyInsuranceMarketEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "In particular, the `step()` method seems to be faulty.\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/environment/environment.py\", line 160, in step\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     market = self.mm.evolve(self.dt)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/market_manager.py\", line 323, in evolve\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     self.market.time = start_time\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m AttributeError: 'NoneType' object has no attribute 'time'\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m ValueError: Your environment (<SpecialtyInsuranceMarketEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    method.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    `reset()` method.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    setting).\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    per-agent dict).\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 414, in __init__\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     check_env(self.env, self.config)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 96, in check_env\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 363, in check_multiagent_environments\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     results = env.step(sampled_action)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/environment/environment.py\", line 160, in step\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     market = self.mm.evolve(self.dt)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/market_manager.py\", line 323, in evolve\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     self.market.time = start_time\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m AttributeError: 'NoneType' object has no attribute 'time'\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     check_multiagent_environments(env)\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m   File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 368, in check_multiagent_environments\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m ValueError: Your environment (<SpecialtyInsuranceMarketEnv instance>) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m In particular, the `step()` method seems to be faulty.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    method.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    `reset()` method.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    setting).\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    per-agent dict).\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=1816)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 363, in check_multiagent_environments\n    results = env.step(sampled_action)\n  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/environment/environment.py\", line 160, in step\n    market = self.mm.evolve(self.dt)\n  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/market_manager.py\", line 323, in evolve\n    self.market.time = start_time\nAttributeError: 'NoneType' object has no attribute 'time'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n    check_multiagent_environments(env)\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 368, in check_multiagent_environments\n    raise ValueError(\nValueError: Your environment (<SpecialtyInsuranceMarketEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `step()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:159\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:229\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_workers_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:616\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py:487\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m     remote_results\u001b[38;5;241m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[38;5;241m=\u001b[39mresult), tag)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:22\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m auto_init_ray()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/_private/worker.py:2626\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2626\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/environment/environment.py\", line 160, in step\n    market = self.mm.evolve(self.dt)\n  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/market_manager.py\", line 323, in evolve\n    self.market.time = start_time\nAttributeError: 'NoneType' object has no attribute 'time'\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n    check_multiagent_environments(env)\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 368, in check_multiagent_environments\n    raise ValueError(\nValueError: Your environment (<SpecialtyInsuranceMarketEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `step()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 414, in __init__\n    check_env(self.env, self.config)\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 96, in check_env\n    raise ValueError(\nValueError: Traceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 363, in check_multiagent_environments\n    results = env.step(sampled_action)\n  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/environment/environment.py\", line 160, in step\n    market = self.mm.evolve(self.dt)\n  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/market_manager.py\", line 323, in evolve\n    self.market.time = start_time\nAttributeError: 'NoneType' object has no attribute 'time'\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1816, ip=127.0.0.1, actor_id=0dadafe8d563d9b1fc6a1c0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x14f021be0>)\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n    check_multiagent_environments(env)\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 368, in check_multiagent_environments\n    raise ValueError(\nValueError: Your environment (<SpecialtyInsuranceMarketEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `step()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      9\u001b[0m     runner \u001b[38;5;241m=\u001b[39m GameRunner(sim_args, manager_args, brokers, syndicates, reinsurancefirms, shareholders, catastrophes, risk_model_configs, with_reinsurance, num_risk_models)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/ai_model/runner.py:200\u001b[0m, in \u001b[0;36mAIRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m num_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    190\u001b[0m insurance_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msim_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_args,\n\u001b[1;32m    191\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanager_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanager_args,\n\u001b[1;32m    192\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrokers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrokers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_reinsurance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_reinsurance,\n\u001b[1;32m    199\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_risk_models\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_risk_models}\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppo_trainer_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsurance_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_training):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m         \u001b[38;5;66;03m# Return the trained trainer\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/ai_model/runner.py:70\u001b[0m, in \u001b[0;36mAIRunner.ppo_trainer_creator\u001b[0;34m(self, insurance_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m     high\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m30000000.0\u001b[39m)\n\u001b[1;32m     51\u001b[0m config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_level\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecialtyInsuranceMarket-validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     },\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: insurance_args}\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:516\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     },\n\u001b[1;32m    514\u001b[0m }\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/tune/trainable/trainable.py:161\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    157\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:638\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Create a set of env runner actors via a WorkerSet.\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rollout_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:181\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# errors.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mactor_init_failed:\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;66;03m# Raise the original error here that the EnvRunners raised\u001b[39;00m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;66;03m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 363, in check_multiagent_environments\n    results = env.step(sampled_action)\n  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/environment/environment.py\", line 160, in step\n    market = self.mm.evolve(self.dt)\n  File \"/Users/yubi/Documents/KCLFirstYear/Simulator-SpecialtyInsurance/manager/market_manager.py\", line 323, in evolve\n    self.market.time = start_time\nAttributeError: 'NoneType' object has no attribute 'time'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\n    check_multiagent_environments(env)\n  File \"/opt/homebrew/lib/python3.8/site-packages/ray/rllib/utils/pre_checks/env.py\", line 368, in check_multiagent_environments\n    raise ValueError(\nValueError: Your environment (<SpecialtyInsuranceMarketEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\nIn particular, the `step()` method seems to be faulty.\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env])."
     ]
    }
   ],
   "source": [
    "# Run the simulation\n",
    "from manager.ai_model.runner import AIRunner\n",
    "from manager.game_model.runner import GameRunner\n",
    "\n",
    "model = 0\n",
    "if model == 0: \n",
    "    runner = AIRunner(sim_args, manager_args, brokers, syndicates, reinsurancefirms, shareholders, catastrophes, risk_model_configs, with_reinsurance, num_risk_models)\n",
    "elif model == 1:\n",
    "    runner = GameRunner(sim_args, manager_args, brokers, syndicates, reinsurancefirms, shareholders, catastrophes, risk_model_configs, with_reinsurance, num_risk_models)\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eef45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.event.add_catastrophe import AddCatastropheEvent\n",
    "from environment.event.add_attritionalloss import AddAttritionalLossEvent\n",
    "from environment.event.add_risk import AddRiskEvent\n",
    "from environment.event.add_premium import AddPremiumEvent\n",
    "from environment.event.add_claim import AddClaimEvent\n",
    "from environment.event_generator import EventGenerator\n",
    "from manager.event_handler import EventHandler\n",
    "from environment.market import NoReinsurance_RiskOne\n",
    "import numpy as np\n",
    "\n",
    "catastrophe_events = EventGenerator(risk_model_configs).generate_catastrophe_events(catastrophes)\n",
    "attritional_loss_events = EventGenerator(risk_model_configs).generate_attritional_loss_events(sim_args, catastrophes)\n",
    "broker_risk_events = EventGenerator(risk_model_configs).generate_risk_events(brokers, catastrophes)\n",
    "broker_premium_events = []\n",
    "broker_claim_events = []\n",
    "time = 0\n",
    "market = NoReinsurance_RiskOne(time, sim_args[\"max_time\"], manager_args, brokers, syndicates, \n",
    "                               shareholders, catastrophes, risk_model_configs, catastrophe_events, \n",
    "                               attritional_loss_events, broker_risk_events, broker_premium_events, \n",
    "                               broker_claim_events)\n",
    "event_handler = EventHandler(sim_args[\"max_time\"], catastrophe_events, attritional_loss_events, broker_risk_events, broker_premium_events, broker_claim_events)\n",
    "\n",
    "step_time = 1\n",
    "market_start_time = market.time\n",
    "market_end_time = market.time + step_time\n",
    "\n",
    "upcoming_catastrophe = [\n",
    "            e.risk_id for e in event_handler.upcoming_catastrophe.values() if isinstance(e, AddCatastropheEvent)\n",
    "        ]\n",
    "upcoming_attritional_loss = [\n",
    "            e.risk_id for e in event_handler.upcoming_attritional_loss.values() if isinstance(e, AddAttritionalLossEvent)\n",
    "        ]\n",
    "upcoming_broker_risk = [\n",
    "            e.risk_id for e in event_handler.upcoming_broker_risk.values() if isinstance(e, AddRiskEvent)\n",
    "        ]\n",
    "upcoming_broker_premium = [\n",
    "            e.risk_id for e in event_handler.upcoming_broker_premium.values() if isinstance(e, AddPremiumEvent)\n",
    "        ]\n",
    "upcoming_broker_claim = [\n",
    "            e.risk_id for e in event_handler.upcoming_broker_claim.values() if isinstance(e, AddClaimEvent)\n",
    "        ]\n",
    "step_time = 1\n",
    "market = event_handler.forward(market, step_time)\n",
    "\n",
    "newly_added_catastrophe_events = {\n",
    "            e.risk_id: e.risk_start_time\n",
    "            for e in event_handler.completed_catastrophe.values()\n",
    "            if isinstance(e, AddCatastropheEvent) and (e.risk_id in upcoming_catastrophe)\n",
    "        }\n",
    "\n",
    "newly_added_attritional_loss_events = {\n",
    "            e.risk_id: e.risk_start_time\n",
    "            for e in event_handler.completed_attritional_loss.values()\n",
    "            if isinstance(e, AddAttritionalLossEvent) and (e.risk_id in upcoming_attritional_loss)\n",
    "        }\n",
    "\n",
    "newly_added_broker_risk_events = {\n",
    "            e.risk_id: e.risk_start_time\n",
    "            for e in event_handler.completed_broker_risk.values()\n",
    "            if isinstance(e, AddRiskEvent) and (e.risk_id in upcoming_broker_risk)\n",
    "        }\n",
    "\n",
    "newly_added_broker_premium_events = {\n",
    "            e.risk_id: e.risk_start_time\n",
    "            for e in event_handler.completed_broker_premium.values()\n",
    "            if isinstance(e, AddPremiumEvent) and (e.risk_id in upcoming_broker_premium)\n",
    "        }\n",
    "\n",
    "newly_added_broker_claim_events = {\n",
    "            e.risk_id: e.risk_start_time\n",
    "            for e in event_handler.completed_broker_claim.values()\n",
    "            if isinstance(e, AddClaimEvent) and (e.risk_id in upcoming_broker_claim)\n",
    "        }\n",
    "\n",
    "catastrophe_event_start_times = np.array(\n",
    "            [\n",
    "                newly_added_catastrophe_events.get(risk_id)\n",
    "                for risk_id in upcoming_catastrophe\n",
    "                if newly_added_catastrophe_events.get(risk_id) != None\n",
    "            ]\n",
    "        )\n",
    "\n",
    "attritional_loss_event_start_times = np.array(\n",
    "            [\n",
    "                newly_added_attritional_loss_events.get(risk_id)\n",
    "                for risk_id in upcoming_attritional_loss\n",
    "                if newly_added_attritional_loss_events.get(risk_id) != None\n",
    "            ]\n",
    "        )\n",
    "\n",
    "broker_risk_event_start_times = np.array(\n",
    "            [\n",
    "                newly_added_broker_risk_events.get(risk_id)\n",
    "                for risk_id in upcoming_broker_risk\n",
    "                if newly_added_broker_risk_events.get(risk_id) != None\n",
    "            ]\n",
    "        )\n",
    "\n",
    "broker_premium_event_start_times = np.array(\n",
    "            [\n",
    "                newly_added_broker_premium_events.get(risk_id)\n",
    "                for risk_id in upcoming_broker_premium\n",
    "                if newly_added_broker_premium_events.get(risk_id) != None\n",
    "            ]\n",
    "        )\n",
    "\n",
    "broker_claim_event_start_times = np.array(\n",
    "            [\n",
    "                newly_added_broker_claim_events.get(risk_id)\n",
    "                for risk_id in upcoming_broker_claim\n",
    "                if newly_added_broker_claim_events.get(risk_id) != None\n",
    "            ]\n",
    "        )\n",
    "\n",
    "event_start_times = np.concatenate((catastrophe_event_start_times,\n",
    "                                    attritional_loss_event_start_times,\n",
    "                                    broker_risk_event_start_times,\n",
    "                                    broker_premium_event_start_times,\n",
    "                                    broker_claim_event_start_times))\n",
    "\n",
    "sorted_unique_start_times = np.sort(np.unique(event_start_times))\n",
    "\n",
    "print(event_handler.completed_attritional_loss.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c98d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
